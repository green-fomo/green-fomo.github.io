<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <!-- CSS  -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
</head>

<body>

<!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      
      <div class="navbar-header">
        <a class="navbar-brand" href="/"></a>
        <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>
  
      <div class="navbar-collapse collapse" id="navbar-main">
        <ul class="nav navbar-nav">
          <li><a href="index.html">About</a></li>
          <li><a href="call.html">Call For Papers</a></li>
          <!--<li><a href="accepted.html">Accepted Papers</a></li>-->
          <li><a href="dates.html">Dates</a></li>
          <li><a href="speakers.html">Invited Speakers</a></li>
          <li><a href="program.html">Program</a></li>
          <li><a href="organizers.html">Organizers</a></li>
          <li><a href="contact.html">Contact</a></li>
        </ul>
      </div>
  
    </div>
  </div>

<div class="container">
  <div class="page-content">
      <p><br /></p>

      <div class="row">
        <div class="col-xs-12">
          <center><h1>Green Foundation Models</h1></center>
          <center><h2>ECCV 2024</h2></center>
          <!-- <center>June 25, 2021</center> -->
        </div>
      </div>

<hr />


<p><br /></p>

<center>
<div class="row" id="call">
  <div class="col-xs-12">
    <h2><u>Call For Papers</u></h2>
  </div>
</div>
</center>


<div class="row">
    <div class="col-xs-12">
      <p>
        Papers must adhere to the ECCV format, with a maximum length of 8 
        pages as per the main conference author guidelines. Each submission will undergo 
        review by a minimum of two reviewers under a double-blind policy. 
        Selection for inclusion will be determined by the paper's relevance, 
        significance, novelty of findings, technical quality, and clarity of 
        presentation. Accepted papers will be featured in the ECCV 2024 workshop proceedings.

        <br></br>
        All the papers should be submitted using CMT website.

        <br></br>
        <b>Relevant topics</b> of interest to the workshop include, but are not limited to:
        <li>Training-free methods</li>
        <li>Parameter efficient fine-tuning</li>
        <li>Model compression and pruning</li>
        <li>Token reduction</li>
        <li>Light-weight models</li>
        <li>Uni-(multi-)modal reasoning</li>
        <li>Continual/transfer learning</li>
        <li>Novel applications for sustainability and biodiversity</li>
        <br></br>
    </p>
    </div>
  </div>


<p><br /></p>


<center>
<div class="row" id="dates">
  <div class="col-xs-12">
    <h2><u>Important Workshop Dates</u></h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped" style="width: 70%; margin: auto;">
      <tbody>
        <tr>
          <td>Submission site opens</td>
          <td> June 12, 2024 </td>
        </tr>
        <tr>
          <td>Submission deadline</td>
          <td>July 12, 2024</td>
        </tr>
        <tr>
          <td>Decisions announced</td>
          <td>August 13, 2024</td>
        </tr>
        <tr>
          <td>Camera-ready due</td>
          <td>August 31, 2024</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>TBD.</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
</center>

<p><br /></p>

<center>
<div class="row" id="call-refs">
  <div class="col-xs-12">
    <h2><u>References</u></h2>
  </div>
</div>
</center>

<div class="row">
  <div class="col-xs-12">
    <ul>
        <li>Bolya, D., Hoffman, J.: Token merging for fast stable diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.4598–4602 (2023)</li>
        <li>Conti, A., Fini, E., Mancini, M., Rota, P., Wang, Y., Ricci, E.: Vocabulary-free image classification. In: NeurIPS (2023)</li>
        <li>Frantar, E., Alistarh, D.: Sparsegpt: Massive language models can be accurately pruned in one-shot. In: International Conference on Machine Learning. pp. 10323–10337. PMLR (2023)</li>
        <li>Gupta, T., Kembhavi, A.: Visual programming: Compositional visual reasoning without training. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 14953–14962 (June 2023)</li>
        <li>Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)</li>
        <li>Jatavallabhula, K., Kuwajerwala, A., Gu, Q., Omama, M., Chen, T., Li, S., Iyer, G., Saryazdi, S., Keetha, N., Tewari, A., Tenenbaum, J., de Melo, C., Krishna, M., Paull, L., Shkurti, F., Torralba, A.: Conceptfusion: Open-set multimodal 3d mapping. arXiv (2023)</li>
        <li>Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S., Hariharan, B., Lim, S.N.: Visual prompt tuning. In: European Conference on Computer Vision. pp. 709–727. Springer (2022) </li>
        <li>Liu, M., Roy, S., Li, W., Zhong, Z., Sebe, N., Ricci, E.: Democratizing fine-grained visual recognition with large language models. In: International Conference on Learning Representations (ICLR) (2024), https://openreview.net/forum?id= c7DND1iIgb</li>
        <li>Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748–8763. PMLR (2021)</li>
        <li>Sun, M., Liu, Z., Bair, A., Kolter, J.Z.: A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695 (2023)</li>
        <li>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)</li>
        <li>Xu, M., Yin, W., Cai, D., Yi, R., Xu, D., Wang, Q., Wu, B., Zhao, Y., Yang, C., Wang, S., et al.: A survey of resource-efficient llm and multimodal foundation models. arXiv preprint arXiv:2401.08092 (2024)</li>
        <li>Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prompt learning for vision-language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16816–16825 (2022)</li>
        <li>Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. International Journal of Computer Vision 130(9), 2337–2348 (2022)</li>
    </ul>
  </div>
</div>
</body>
</html>